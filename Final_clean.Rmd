---
title: "ph244 Final Project"
author: "Antonia Gibbs"
date: "5/3/2022"
output: html_document
---
```{r Packages and data, message = FALSE, warning = FALSE}
library(randomForest)
library(randomForestExplainer)
library(data.table)
library(ggfortify)
library(dplyr)
library(ggplot2)
library(tictoc)
library(datasets)
library(caret)
library(glmnet)


# reading in data
traindat_all = fread("bioresponse/train.csv", header=T)
#testdat = fread("bioresponse/test.csv", header=T)
```

```{r Split Train and Test}
set.seed(244)

data_shuffled <- sort(sample(nrow(traindat_all), nrow(traindat_all)*.75))
train <- traindat_all[data_shuffled,] %>% data.frame()
test <- traindat_all[-data_shuffled,] %>% data.frame()

```



```{r}
# removing outcome
train_pca = train %>% select(-c(Activity))
# identifying columns with 0 variance
which(apply(train_pca, 2, var)==0)
which(apply(test, 2, var)==0)
# dropping columns with 0 variance in train data
train_pca = train_pca[ , which(apply(train_pca, 2, var) != 0)]
```


```{r PCA}
pca = prcomp(train_pca, scale.=TRUE)
summary(pca)
# # https://cran.r-project.org/web/packages/ggfortify/vignettes/plot_pca.html
# autoplot(pca)
# ggplot2 way - w coloring
dtp <- data.frame('Activity' = train$Activity, pca$x[,1:2]) # the first two components are selected (NB: you can also select 3 for 3D plotting or 3+)
ggplot(data = dtp) + 
       geom_point(aes(x = PC1, y = PC2, col = Activity)) + 
       theme_minimal() 
```


```{r PCA Visuals}
#calculate total variance explained by each principal component
var_explained = pca$sdev^2 / sum(pca$sdev^2)
# scree plot of all principal components
qplot(c(1:1776), var_explained) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("Variance Explained") +
  ggtitle("Scree Plot") #+
  # ylim(0, 1)
# scree plot of first 5 principal components
qplot(c(1:5), var_explained[1:5]) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("Variance Explained") +
  ggtitle("Figure 1: Scree Plot of first 5 PC") + theme_bw() + theme(plot.title = element_text(hjust = 0.5))#+
  # ylim(0, 1)
```


```{r, eval=F, include=F}
sm = 0
counter = 1
while (sm < 0.9){
  sm = sm + var_explained[counter]
  counter = counter + 1
}
counter
var_explained[1:counter] %>% sum()
```


```{r Log Loss Metric Function}

LogLoss<-function(actual, predicted)
{
result<- -1/length(actual)*(sum((actual*log(predicted)+(1-actual)*log(1-predicted))))
return(result)
}
```


```{r Logistic Regression with Penalty}
#Make x a matrix and y a vector for both testing and training
y_train <- train$Activity
x_train <- as.matrix(train %>% select(-Activity))

y_test <- test$Activity
x_test <- as.matrix(test %>% select(-Activity))

#Prediction evaluator function
Prediction <- function(model)
{ pred_probs <- model %>% predict(newx = x_test, type = "response")
  binary_pred <- ifelse(pred_probs > 0.5, 1, 0)
  histogram(binary_pred)
  logloss <- LogLoss(y_test, pred_probs)
  accuracy <- mean(binary_pred == y_test)
  return(table(y_test, binary_pred))
}

#elastic net
lr.elastic <- glmnet(y = y_train, x = x_train, alpha = 0.5, family = "binomial", lambda = 0.01)
#ridge regression
lr.ridge <- glmnet(y = y_train, x = x_train, alpha = 0, family = "binomial", lambda = 0.1)
#lasso 
lr.lasso <- glmnet(y = y_train, x = x_train, alpha = 1, family = "binomial", lambda = 0.1)

```


```{r Random Forest, eval = F}
set.seed(244)
mod.rf <- randomForest(Activity~., data=train, proximity=TRUE, ntree = 125, localImp = TRUE, nodesize = 1)
                       #importance = T, maxnodes = 200, nodesize = 1, sampsize = 400)
pred_rf <- mod.rf %>% predict(newdata = test)
#save(pred_rf, file = "pred_rf.rda")
```

```{r Random Forest Visuals}
#min_depth_frame <- min_depth_distribution(mod.rf)
#save(min_depth_frame, file = "min_depth_frame.rda")
load("pred_rf.rda")
load("min_depth_frame.rda")
#head(min_depth_frame, n = 10)
plot_min_depth_distribution(min_depth_frame, main = "Figure 2: Distribution of Minimal Depth and its Mean from RF Model",
                            mean_sample = "top_trees") + 
  theme_bw() + theme(plot.title = element_text(hjust = 0.5))


#importance_frame <- measure_importance(mod.rf)
#save(importance_frame, file = "importance_frame.rda")
load("importance_frame.rda")
#importance_frame
plot_multi_way_importance(importance_frame, size_measure = "no_of_nodes", #no_of_labels = 5,
                          main = "Figure 3: Multi-way Importance Plot of RF Model") + theme_bw() +
                          theme(plot.title = element_text(hjust = 0.5))
vars <- important_variables(importance_frame, k = 5, measures = c("mean_min_depth", "no_of_trees"))
#head(vars)
```


```{r}
##Take out variables that have multicollinearity 

#CRASHES
##cor_df <- cor(train, use = "complete.obs")
#cor_df[upper.tri(cor_df)] <- 0 
#diag(cor_df) <- 0
#train_nocor <- 
#  train[, !apply(cor_df, 2, function(x) any(abs(x) > 0.75, na.rm = TRUE))]
#dim(train_nocor)
```


```{r Support Vector Machine }
train$Activity <- factor(train$Activity, levels = c(0, 1))
library(e1071)
mod.svm <- svm(formula = Activity ~ .,
                 data = train,
                 type = 'C-classification', kernel = "linear")
pred_svm <- mod.svm %>% predict(newdata = test, type = "response")

#Accuracy is 73.24% with linear kernel  idk why the mean doesn't work 
#Visualization 
#plot(mod.svm, train, formula = Activity ~.)

```




```{r Accuracy Precision Recall Function}
APR <- function(cm, model_name) { 
  tp <- cm[2,2]
  fp <- cm[1,2]
  tn <- cm[1,1]
  fn <- cm[2,1]
  acc <- (tp + tn) / (tp + fp + tn + fn)
  prec <- tp / (tp + fp)     #Precision (TP / TP + FP) or (d/d+b)
  rec <-  tp / (tp + fn)       #Recall (TP / TP + FN)
  print(paste0("Classification metrics for ", model_name))
  print(paste0("Accuracy is ", round(acc, digits = 3)))
  print(paste0("Precision is ", round(prec, digits = 3)))
  print(paste0("Recall is ", round(rec, digits = 3)))
  }

APR(table(y_test, pred_rf), "Random Forest Regression")
APR(table(y_test, pred_svm), "Support Vector Machine")
APR(Prediction(lr.elastic), "Elastic Net Regression")
APR(Prediction(lr.ridge), "Ridge Regression")
APR(Prediction(lr.lasso), "LASSO Penalty")

```
